{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multiple_regression',\n",
       "  'what_is multiple regression',\n",
       "  'Multiple regression is an extension of simple linear regression. It is used when \\n                we want to predict the value of a variable based on the value of two or more other variables'),\n",
       " ('multiple_regression',\n",
       "  'how_to_intepret multiple regression',\n",
       "  'Regression coefficients represent the mean change in the response variable for one unit of\\n                  change in the predictor variable while holding other predictors in the model constant'),\n",
       " ('multiple_regression',\n",
       "  'how_to_apply multiple regression',\n",
       "  ' You can use R base linear model function(lm) to fit the model.\\n\\n                   fit <- lm(y ~ x1 + x2 + x3, data=mydata)'),\n",
       " ('multiple_regression',\n",
       "  'assess_accuracy_model multiple regression',\n",
       "  'I suggest you use RMSE to assess the accuracy, which is the square root of the variance of the residuals. It indicates the \\n               absolute fit of the model to the data and how close the observed data points are to the model’s predicted\\n               values. RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful \\n               property of being in the same units as the response variable. Lower values of RMSE indicates a better fit. \\n\\n               rmse <-  rmse(actual, predicted) '),\n",
       " ('multiple_regression',\n",
       "  'diagnostics multiple regression',\n",
       "  \"R-squared - It's a measure of percentage of variance of response is explained by the predictors.\\n\\n                Adjusted R-squared - incorporates the model’s degrees of freedom. Can assess multiple models with differnet number of predictors\\n                RMSE - Square root of the average of squared errors. this is the best measure to use\"),\n",
       " ('multiple_regression',\n",
       "  'model_selection_methods multiple regression',\n",
       "  'Forward selection -Forward selection begins with an empty equation.  Predictors are added one at a time beginning with the \\n                                       predictor with the highest correlation with the dependent variable. /n\\n\\n                    Backward elimination - All the independent variables are entered into the equation first and each one is deleted one at a time if \\n                                        they do not contribute to the regression equation /n\\n\\n                    Stepwise selection - involves analysis at each step to determine the contribution of the predictor variable entered previously in the equation.  \\n                                        So it is possible to understand the contribution of the previous variables now that another variable has been added.'),\n",
       " ('multiple_regression',\n",
       "  'Multicollinearity multiple regression',\n",
       "  'multicollinearity results in unstable parameter estimates which makes it very difficult to assess the effect of independent variables on dependent variables. \\n                The variance of the parameter estimates can be larger. Advanced regression methods like Ridge regression and Lasso can be used as solution for this matter. '),\n",
       " ('multiple_regression',\n",
       "  'Assumptions multiple regression',\n",
       "  'Here are the assumptions of linear regression \\n\\n\\n                1) linearity and additivity of the relationship between dependent and independent variables \\n\\n                2) statistical independence of the errors \\n\\n                3) homoscedasticity (constant variance) of the errors \\n\\n                4) Normality of the error distribution. \\n                '),\n",
       " ('multiple_regression',\n",
       "  'improve_accuracy multiple regression',\n",
       "  'there are multiple ways to gain improvements in the prediction model, having sufficient domain knowledge (context) will also give you the best possible chance of \\n                   getting improvements. Here are a list of things for you to do... \\n\\n\\n                Do feature engineering to identiy the best possible variables \\n\\n                Normalize or standardize your data \\n\\n                Try fitting a different regression model (eg. polynomial or possion regression) \\n\\n                Use an advanced regression method like Ridge regression or Lasso \\n\\n                '),\n",
       " ('multiple_regression',\n",
       "  'non_linear_relationships multiple regression',\n",
       "  'Poisson regression is a great tool to use if you have non linear relationships in the data. To use poisson regression in R, you can use the glm package \\n. \\n                    model <- glm(num_awards ~ prog + math, family=\"poisson\", data=p) '),\n",
       " ('multiple_regression',\n",
       "  'use_categorical_variables multiple regression',\n",
       "  'Any analysis requires numerical variables. So, when you wish to include a categorical variable in a model, \\n              they are recoded into a set of separate binary variables. This recoding is called “dummy coding” and leads to the \\n              creation of a table called contrast matrix. This is done automatically by  R'),\n",
       " ('multiple_regression',\n",
       "  'outliers multiple regression',\n",
       "  'Machine learning algorithms are very sensitive to the range and distribution of attribute values. Data outliers can spoil and mislead the training process \\n                  resulting in longer training times, less accurate models and ultimately poorer results. I suggest you use one of the following procedures to deal with outliers.\\n\\n                    Univariate method: This method looks for data points with extreme values on one variable \\n\\n                    Multivariate method: Here we look for unusual combinations on all the variables \\n\\n                    Minkowski error: This method reduces the contribution of potential outliers in the training process \\n\\n                    '),\n",
       " ('decision_tree',\n",
       "  'what_is decision tree',\n",
       "  'The idea of a decision tree is to divide the data set into smaller data sets based on \\n                the descriptive features until you reach a small enough set that contains data points that fall under one label.'),\n",
       " ('decision_tree',\n",
       "  'how_to_intepret decision tree',\n",
       "  ' In the tree, each node represents a feature(attribute), each link(branch) represents a decision(rule) \\n                   and each leaf represents an outcome(categorical or continues value) '),\n",
       " ('decision_tree',\n",
       "  'how_to_apply decision tree',\n",
       "  \" You can use R package 'tree' fit the model.\\n\\n               tree.model = tree(y~ x1 + x2, data=mydata)\"),\n",
       " ('decision_tree',\n",
       "  'assess_accuracy_model decision tree',\n",
       "  '\\n               Try calculating the confusion matrix first: \\n\\n               confMat <- table(test$class,t_pred) \\n\\n                Now you can calculate the accuracy by dividing the sum diagonal of the matrix - which are \\n                the correct predictions - by the total sum of the matrix: \\n\\n                accuracy <- sum(diag(confMat))/sum(confMat) '),\n",
       " ('decision_tree',\n",
       "  'diagnostics decision tree',\n",
       "  'A ROC curve is great tool for assessing binary classification models, and since yours is a multi-class\\n                   classification, I suggest you use the confusion matrix as your primary diagnostic tool'),\n",
       " ('decision_tree',\n",
       "  'model_selection_methods decision tree',\n",
       "  ' decision trees tend to be highly overfitting. It happens when the learning algorithm \\n                    continues to develop hypotheses that reduce training set error at the cost of an\\n                    increased test set error. There are several approaches to avoid overfitiing and select the best model \\n\\n                Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set.\\n\\n                Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree. '),\n",
       " ('decision_tree',\n",
       "  'use_categorical_variables decision tree',\n",
       "  'Any analysis requires numerical variables. So, when you wish to include a categorical variable in a model, \\n              they are recoded into a set of separate binary variables. This recoding is called “dummy coding” and leads to the \\n              creation of a table called contrast matrix. This is done automatically by  R'),\n",
       " ('decision_tree',\n",
       "  'outliers decision tree',\n",
       "  'Machine learning algorithms are very sensitive to the range and distribution of attribute values. Data outliers can spoil and mislead the training process \\n                  resulting in longer training times, less accurate models and ultimately poorer results. I suggest you use one of the following procedures to deal with outliers.\\n\\n                    Univariate method: This method looks for data points with extreme values on one variable \\n\\n                    Multivariate method: Here we look for unusual combinations on all the variables \\n\\n                    Minkowski error: This method reduces the contribution of potential outliers in the training process \\n\\n                    '),\n",
       " ('decision_tree',\n",
       "  'Multicollinearity decision tree',\n",
       "  'Desicion trees make no assumptions on relationships between features. It just constructs splits on single features that improves classification, based on an \\n                    impurity measure like Gini or entropy. If features A, B are heavily correlated, no /little information can be gained from splitting on B after having split on A. \\n                    So it would typically get ignored in favor of C. '),\n",
       " ('multiple_regression',\n",
       "  'improve_accuracy decision tree',\n",
       "  'Decision trees might be performing low because of many reasons, one prominent reason which I can think of is that while calculating a split they do not consider \\n                  inter-dependency of variables or of target variable on other variables\\n\\n\\n                1. Variable preselection: Different tests can be done like multicollinearity test, VIF calculation on variables to select only a few top variables. This will lead in improved performance as it would strictly cut out the undesired variables.\\n\\n                2. Ensemble Learning Use multiple trees (random forests) to predict the outcomes. Random forests in general perform well than a single decision tree as they manage to reduce both bias and variance. They are less prone to overfitting as well.\\n\\n                3. K-Fold cross validation: Cross validation in the training data itself can improve the performance of the model a bit.\\n\\n                4. Hybrid Model: Use a hybrid model, i.e. use logistic regression after using decision trees to improve performance.\\n\\n                '),\n",
       " ('multiple_regression',\n",
       "  'Assumptions decision tree',\n",
       "  \"A decision tree is a largely used non-parametric effective machine learning. \\n\\n                  Therefore there aren't any underlying assumptions about the distribution of the errors or the data.\"),\n",
       " ('multiple_regression',\n",
       "  'non_linear_relationships decision tree',\n",
       "  'Decision trees is a non-linear classifier like the neural networks, etc. It is generally used for classifying non-linearly separable data.\\n                    Even when you consider the regression example, decision tree is non-linear. ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('qaCheck.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "# c.execute(\"\"\"CREATE TABLE q_a(\n",
    "#              category text,\n",
    "#              question text,\n",
    "#              answer text)\"\"\")\n",
    "\n",
    "class Questions:\n",
    "    \"\"\"A sample questions class\"\"\"\n",
    "    def __init__(self, category, question, answer):\n",
    "        self.category = category\n",
    "        self.question = question\n",
    "        self.answer = answer \n",
    "        \n",
    "def insert_QandA(question):\n",
    "    with conn:\n",
    "        c.execute(\"INSERT INTO q_a VALUES(:category, :question, :answer)\",\n",
    "                 {'category':question.category, 'question':question.question, 'answer':question.answer})\n",
    "        \n",
    "def remove_QandA(question):\n",
    "    with conn:\n",
    "        c.execute(\"DELETE FROM q_a WHERE question = :question\",\n",
    "                 {'question':question.question})\n",
    "        \n",
    "def update_pay(qobj, answerr):\n",
    "    with conn:\n",
    "        c.execute(\"UPDATE q_a SET answer = :answer WHERE question = :question\",\n",
    "                 {'question':qobj.question, 'answer': answerr})\n",
    "\n",
    "def get_ques_by_category(category):\n",
    "    c.execute('SELECT * FROM q_a WHERE category = :category',{'category':category})\n",
    "    return c.fetchall()    \n",
    "\n",
    "\n",
    "q1 = Questions('decision_tree', 'what_is decision tree',\n",
    "               \"\"\"The idea of a decision tree is to divide the data set into smaller data sets based on \n",
    "                the descriptive features until you reach a small enough set that contains data points that fall under one label.\"\"\")\n",
    "\n",
    "q2 = Questions('decision_tree', 'how_to_intepret decision tree',\n",
    "               \"\"\" In the tree, each node represents a feature(attribute), each link(branch) represents a decision(rule) \n",
    "                   and each leaf represents an outcome(categorical or continues value) \"\"\")\n",
    "\n",
    "q3 = Questions('decision_tree', 'how_to_apply decision tree',\n",
    "               \"\"\" You can use R package 'tree' fit the model.\\n\n",
    "               tree.model = tree(y~ x1 + x2, data=mydata)\"\"\")\n",
    "\n",
    "q4 = Questions('decision_tree', 'assess_accuracy_model decision tree',\n",
    "               \"\"\"\n",
    "               Try calculating the confusion matrix first: \\n\n",
    "               confMat <- table(test$class,t_pred) \\n\n",
    "                Now you can calculate the accuracy by dividing the sum diagonal of the matrix - which are \n",
    "                the correct predictions - by the total sum of the matrix: \\n\n",
    "                accuracy <- sum(diag(confMat))/sum(confMat) \"\"\")\n",
    "\n",
    "q5 = Questions('decision_tree', 'diagnostics decision tree',\n",
    "               \"\"\"A ROC curve is great tool for assessing binary classification models, and since yours is a multi-class\n",
    "                   classification, I suggest you use the confusion matrix as your primary diagnostic tool\"\"\")\n",
    "\n",
    "q6 = Questions('decision_tree', 'model_selection_methods decision tree',\n",
    "                 \"\"\" decision trees tend to be highly overfitting. It happens when the learning algorithm \n",
    "                    continues to develop hypotheses that reduce training set error at the cost of an\n",
    "                    increased test set error. There are several approaches to avoid overfitiing and select the best model \\n\n",
    "                Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set.\\n\n",
    "                Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree. \"\"\" )\n",
    "\n",
    "\n",
    "q7 = Questions(\"decision_tree\",'use_categorical_variables decision tree',\n",
    "              \"\"\"Any analysis requires numerical variables. So, when you wish to include a categorical variable in a model, \n",
    "              they are recoded into a set of separate binary variables. This recoding is called “dummy coding” and leads to the \n",
    "              creation of a table called contrast matrix. This is done automatically by  R\"\"\")\n",
    "\n",
    "q8 = Questions(\"decision_tree\", 'outliers decision tree',\n",
    "              \"\"\"Machine learning algorithms are very sensitive to the range and distribution of attribute values. Data outliers can spoil and mislead the training process \n",
    "                  resulting in longer training times, less accurate models and ultimately poorer results. I suggest you use one of the following procedures to deal with outliers.\\n\n",
    "                    Univariate method: This method looks for data points with extreme values on one variable \\n\n",
    "                    Multivariate method: Here we look for unusual combinations on all the variables \\n\n",
    "                    Minkowski error: This method reduces the contribution of potential outliers in the training process \\n\n",
    "                    \"\"\")\n",
    "\n",
    "q9 = Questions('decision_tree', 'Multicollinearity decision tree',\n",
    "                \"\"\"Desicion trees make no assumptions on relationships between features. It just constructs splits on single features that improves classification, based on an \n",
    "                    impurity measure like Gini or entropy. If features A, B are heavily correlated, no /little information can be gained from splitting on B after having split on A. \n",
    "                    So it would typically get ignored in favor of C. \"\"\")\n",
    "\n",
    "q10 = Questions('multiple_regression', 'improve_accuracy decision tree',\n",
    "               \"\"\"Decision trees might be performing low because of many reasons, one prominent reason which I can think of is that while calculating a split they do not consider \n",
    "                  inter-dependency of variables or of target variable on other variables\\n\n",
    "\n",
    "                1. Variable preselection: Different tests can be done like multicollinearity test, VIF calculation on variables to select only a few top variables. This will lead in improved performance as it would strictly cut out the undesired variables.\\n\n",
    "                2. Ensemble Learning Use multiple trees (random forests) to predict the outcomes. Random forests in general perform well than a single decision tree as they manage to reduce both bias and variance. They are less prone to overfitting as well.\\n\n",
    "                3. K-Fold cross validation: Cross validation in the training data itself can improve the performance of the model a bit.\\n\n",
    "                4. Hybrid Model: Use a hybrid model, i.e. use logistic regression after using decision trees to improve performance.\\n\n",
    "                \"\"\")\n",
    "\n",
    "q11 = Questions('multiple_regression', 'Assumptions decision tree',\n",
    "               \"\"\"A decision tree is a largely used non-parametric effective machine learning. \\n\n",
    "                  Therefore there aren't any underlying assumptions about the distribution of the errors or the data.\"\"\")\n",
    "\n",
    "q12 = Questions('multiple_regression', 'non_linear_relationships decision tree', \n",
    "                \"\"\"Decision trees is a non-linear classifier like the neural networks, etc. It is generally used for classifying non-linearly separable data.\n",
    "                    Even when you consider the regression example, decision tree is non-linear. \"\"\")\n",
    "\n",
    "#insert_QandA(q12)\n",
    "#remove_QandA(q7)\n",
    "#c.execute('DROP TABLE q_a')\n",
    "c.execute('SELECT * FROM q_a')\n",
    "(c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
